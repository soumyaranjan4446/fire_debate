import re
from uuid import uuid4
from typing import List, Optional
from dataclasses import dataclass

from fire_debate.schemas.debate import DebateTurn, Stance, Phase
from fire_debate.schemas.evidence import EvidenceDoc
from fire_debate.rag.retriever import EvidenceRetriever
from fire_debate.agents.librarian import Librarian
from fire_debate.agents.base import LLMClient

@dataclass
class AgentConfig:
    name: str
    stance: Stance
    style: str = "logical" 

class DebaterAgent:
    def __init__(
        self,
        config: AgentConfig,
        llm: LLMClient,
        retriever: EvidenceRetriever,
        librarian: Librarian
    ):
        self.cfg = config
        self.llm = llm
        self.retriever = retriever
        self.librarian = librarian

    def _clean_output(self, text: str) -> str:
        """Helper to strip 'Chatty' prefixes often generated by SLMs."""
        patterns = [
            r"^Here is the .*?:", 
            r"^Response:", 
            r"^Query:", 
            r"^Search:", 
            r"^Answer:", 
            r"^\[.*?\]",  # Remove bracketed thinking if leaked
        ]
        cleaned = text
        for p in patterns:
            cleaned = re.sub(p, "", cleaned, flags=re.IGNORECASE).strip()
        return cleaned.strip('" ')

    def _generate_agentic_query(self, claim: str, history: List[DebateTurn]) -> str:
        """
        Generates a search query. Uses Few-Shot prompting to guide SLMs.
        """
        if not history:
            # Heuristic: For the first turn, search the core entities of the claim
            return claim[:100]

        last_turn = history[-1]
        
        prompt = f"""
        [TASK]
        You are a Fact-Checker. You need to verify a specific claim made by an opponent.
        Generate a Google Search Query (max 8 words) to find evidence.
        
        [EXAMPLES]
        Opponent: "The moon landing was faked by Stanley Kubrick."
        Query: moon landing stanley kubrick evidence
        
        Opponent: "GDP of France dropped by 50% in 2023."
        Query: France GDP statistics 2023 drop
        
        [CURRENT CONTEXT]
        Topic: "{claim[:200]}..."
        Opponent argued: "{last_turn.text[:300]}..."
        
        [OUTPUT]
        Write ONLY the query. Do not write "Here is the query".
        Query:
        """
        
        query = self.llm.generate("You are a search engine tool.", prompt)
        query = self._clean_output(query)
        
        # Fallback for empty/failed generation
        if len(query) < 3: 
            return claim[:50]
            
        return query[:100] # Hard truncate for safety

    def _format_evidence(self, docs: List[EvidenceDoc]) -> str:
        if not docs:
            return "No specific evidence found."
        
        text = ""
        for d in docs:
            # SLMs get confused by too much text. We keep snippets short.
            text += f"- [{d.doc_id}] {d.snippet[:200]} (Source Reliability: {d.reliability_score:.2f})\n"
        return text

    def act(self, claim: str, history: List[DebateTurn], phase: Phase, moderator_instruction: Optional[str] = None) -> DebateTurn:
        print(f"ü§î {self.cfg.name} is thinking...")

        # --- 1. Agentic Search ---
        query = self._generate_agentic_query(claim, history)
        print(f"   üß† {self.cfg.name} Search: '{query}'")

        web_docs = self.retriever.search_web(query)
        self.retriever.index_documents(web_docs)
        raw_evidence = self.retriever.retrieve_context(query)
        trusted_evidence = self.librarian.filter_evidence(raw_evidence)
        
        # --- 2. Strategy Selection (Neuro-Symbolic Logic) ---
        if not trusted_evidence:
            print(f"   ‚ö†Ô∏è No evidence. Strategy: Logical Rebuttal.")
            strategy = "CRITICAL: Search found NO evidence. Do not hallucinate citations. Instead, attack the logical flaws, assumptions, or lack of proof in the opponent's argument."
        else:
            strategy = "Use the provided EVIDENCE to support your point. Cite sources as [EVID:doc_id]."

        # --- 3. Argument Generation ---
        # We construct a 'Structured Prompt' that works well for both models
        
        # Get last 2 turns for immediate context
        recent_history = "\n".join([f"{t.agent_id}: {t.text}" for t in history[-2:]])
        
        # Inject Moderator Instruction if it exists (Critical for flow control)
        mod_text = ""
        if moderator_instruction:
            mod_text = f"\n[MODERATOR ORDER]: {moderator_instruction}\n(You MUST obey this order.)"

        system_prompt = (
            f"You are {self.cfg.name}, a debater arguing {self.cfg.stance}.\n"
            f"Style: {self.cfg.style}.\n"
            "GOAL: Deconstruct the opponent's argument. Be sharp and persuasive."
        )
        
        user_prompt = f"""
        [TOPIC]
        {claim[:300]}...

        [IMMEDIATE HISTORY]
        {recent_history}

        [RETRIEVED EVIDENCE]
        {self._format_evidence(trusted_evidence)}

        [INSTRUCTIONS]
        1. {strategy}
        2. Keep response under 150 words.
        3. Do not repeat previous points.{mod_text}

        [RESPONSE]
        """

        response = self.llm.generate(system_prompt, user_prompt)
        cleaned_response = self._clean_output(response)
        
        return DebateTurn(
            turn_id=str(uuid4())[:8],
            agent_id=self.cfg.name,
            stance=self.cfg.stance,
            phase=phase,
            text=cleaned_response,
            citations=re.findall(r"\[EVID:(.*?)\]", cleaned_response),
            search_query=query
        )