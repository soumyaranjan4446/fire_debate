import re
from typing import List, Optional, Dict, Any

# Simple Config Class
class AgentConfig:
    def __init__(self, name, stance, style="logical"):
        self.name = name
        self.stance = stance  # "PRO" or "CON"
        self.style = style

class DebaterAgent:
    def __init__(self, config: AgentConfig, llm_client, retriever, librarian):
        self.cfg = config
        self.name = config.name
        self.stance = config.stance
        self.llm = llm_client
        self.retriever = retriever
        self.librarian = librarian
        self.history = [] 

    def _clean_output(self, text: str) -> str:
        """Helper to strip 'Chatty' prefixes often generated by SLMs."""
        patterns = [
            r"^Here is the .*?:", r"^Response:", r"^Query:", 
            r"^Search:", r"^Answer:", r"^\[.*?\]"
        ]
        cleaned = text
        for p in patterns:
            cleaned = re.sub(p, "", cleaned, flags=re.IGNORECASE).strip()
        return cleaned.strip('" ')

    def _generate_search_query(self, claim: str, phase: str) -> str:
        """Generates a search query based on the current debate phase."""
        prompt = (
            f"You are {self.name} ({self.stance}). Phase: {phase}.\n"
            f"Topic: '{claim}'.\n"
            f"Task: Generate a Google Search Query (max 8 words) to find evidence.\n"
            f"Output ONLY the query string."
        )
        # Use the unified generate method
        query = self.llm.generate(prompt, max_new_tokens=20)
        query = self._clean_output(query)
        
        if len(query) < 3: return claim[:50]
        return query

    def _format_evidence(self, docs: List[Dict[str, Any]]) -> str:
        if not docs:
            return "No specific evidence found."
        
        text = ""
        for i, d in enumerate(docs):
            content = d.get('text', d.get('content', ''))
            score = d.get('score', 0.0)
            text += f"-[{i+1}] {content[:300]}... (Reliability: {score:.2f})\n"
        return text

    def act(self, claim: str, round_num: int, phase: str, opponent_last_arg: str = None):
        # 1. Dynamic Phase Instructions
        # This switches the Agent's "Brain Mode"
        instructions_map = {
            "OPENING": "Make a constructive opening speech defining your key arguments.",
            "REBUTTAL": "Deconstruct the opponent's argument point-by-point. Be aggressive.",
            "CROSS_EX_ASK": "Ask a single, short, trapping question to expose a flaw. Do NOT make a speech.",
            "CROSS_EX_ANSWER": "Answer the question directly, but immediately pivot back to your strength.",
            "CLOSING": "Summarize the debate. CRITICAL: You MUST claim you won. NEVER admit the opponent is right, even if they are. Double down on your stance."
        }
        current_instruction = instructions_map.get(phase, "Argue your side.")

        # 2. Agentic Search
        # We skip search for 'CROSS_EX_ANSWER' to simulate rapid-fire responses
        if phase != "CROSS_EX_ANSWER":
            query = self._generate_search_query(claim, phase)
            # --- DEBUG PRINT ---
            print(f"\n[DEBUG] {self.name} Search Query: '{query}'")
            # -------------------
            raw_evidence = self.retriever.retrieve(query) 
            trusted_evidence = self.librarian.filter_evidence(raw_evidence, claim_context=claim)

            # --- DEBUG PRINT ---
            print(f"[DEBUG] Evidence Found: {len(trusted_evidence)} docs")
            # -------------------
        else:
            trusted_evidence = [] # Rely on memory/logic for answers

        # 3. Build Strategy
        if not trusted_evidence and phase not in ["CROSS_EX_ASK", "CROSS_EX_ANSWER"]:
            strategy = "Search found NO evidence. Attack logical flaws instead."
        else:
            strategy = "Use the provided EVIDENCE to support your point."

        # 4. Construct Prompt
        system_part = (
            f"You are {self.name}, a professional debater.\n"
            f"Stance: {self.stance} ({'Argue TRUE' if self.stance == 'PRO' else 'Argue FALSE'}).\n"
            f"Phase: {phase}.\n"
            f"CRITICAL GOAL: {current_instruction}"
        )
        
        context_part = f"CLAIM: {claim}\n"
        if trusted_evidence:
            context_part += f"\nEVIDENCE FOUND:\n{self._format_evidence(trusted_evidence)}\n"
        
        if opponent_last_arg:
            prefix = "QUESTION ASKED" if phase == "CROSS_EX_ANSWER" else "OPPONENT SAID"
            context_part += f"\n{prefix}: {opponent_last_arg}\n"
            instruction = f"Respond effectively. {strategy}"
        else:
            instruction = f"Start strong. {strategy}"

        full_prompt = f"{system_part}\n\n{context_part}\nINSTRUCTION: {instruction}\n\nRESPONSE:"

        # 5. Generate
        # Use fewer tokens for Cross-Ex to keep it punchy
        max_tokens = 150 if "CROSS" in phase else 512
        
        response = self.llm.generate(full_prompt, max_new_tokens=max_tokens)
        cleaned_response = self._clean_output(response)
        
        self.history.append(cleaned_response)
        
        # --- FIX: Return BOTH text and evidence to satisfy DebateManager ---
        return cleaned_response, trusted_evidence